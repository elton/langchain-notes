{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7340024c-a336-4072-88f7-369d660720cf",
   "metadata": {},
   "source": [
    "# Embedding for Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c2bac-9243-4d5b-9168-f58f34393471",
   "metadata": {},
   "source": [
    "## 使用 [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)\n",
    "\n",
    "This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese.\n",
    "\n",
    "It maps sentences to a 768 dimensional dense vector space and can be used for tasks like sentence embeddings, text matching or semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29395105-ce56-4fea-825d-da15d59cb606",
   "metadata": {},
   "source": [
    "### 使用 text2vec 简化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb74cc-78d2-4670-806f-4c9796161f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U text2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da40cc96-f290-42b8-8dc3-42e0bfab05e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-06-09 14:25:01.458\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtext2vec.sentence_model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m74\u001b[0m - \u001b[34m\u001b[1mUse device: cpu\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.44250618e-04 -2.97346562e-01  8.57901335e-01 ... -5.27700007e-01\n",
      "  -1.43155485e-01 -1.00078315e-01]\n",
      " [ 6.53620481e-01 -7.66668767e-02  9.59623873e-01 ... -6.01224482e-01\n",
      "  -1.67903851e-03  2.14575961e-01]]\n"
     ]
    }
   ],
   "source": [
    "from text2vec import SentenceModel\n",
    "sentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n",
    "\n",
    "model = SentenceModel('shibing624/text2vec-base-chinese')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a6a35-a6c6-49e9-9b11-b9551a6b4b4a",
   "metadata": {},
   "source": [
    "### 使用 HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163776d6-b782-4d39-b3cd-a67695fb06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ！pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12679bf1-3507-4e5b-868f-477f1a9bba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-4.4425e-04, -2.9735e-01,  8.5790e-01,  ..., -5.2770e-01,\n",
      "         -1.4316e-01, -1.0008e-01],\n",
      "        [ 6.5362e-01, -7.6667e-02,  9.5962e-01,  ..., -6.0122e-01,\n",
      "         -1.6790e-03,  2.1458e-01]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = BertTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\n",
    "model = BertModel.from_pretrained('shibing624/text2vec-base-chinese')\n",
    "sentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "# Perform pooling. In this case, mean pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd264877-4c03-4412-a290-f39bf14894d9",
   "metadata": {},
   "source": [
    "### 更换模型为 [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "348ee321-4a6e-4830-ae1e-e4f46f2424c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-0.5050, -0.1925,  0.5590,  ...,  0.8610, -0.7712,  0.7617],\n",
      "        [-0.6504,  0.1314,  0.5595,  ...,  1.0802, -0.4565,  0.7547]])\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = BertTokenizer.from_pretrained('GanymedeNil/text2vec-large-chinese')\n",
    "model = BertModel.from_pretrained('GanymedeNil/text2vec-large-chinese')\n",
    "sentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "# Perform pooling. In this case, mean pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n",
    "\n",
    "print(len(sentence_embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b0220-8211-4b32-8292-81cbdacc033f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
