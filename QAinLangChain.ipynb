{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c051b1-cfa6-447f-815d-a4d5811cfa0d",
   "metadata": {},
   "source": [
    "# 4 Ways to Do Question Answering in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb11915-7ded-4479-b5cc-99ea6d29f441",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2e8b6f-0852-4ffa-81c5-b927dad02326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"data/example.pdf\") # 55 pages document\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ea9aa-4d00-4238-a27d-8c67b0adaa88",
   "metadata": {},
   "source": [
    "## Method 1: load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52374e04-ef9e-4788-b0c2-b7954f69e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 77515e6e1a1fcadaf0d0460f5fea1b5a in your message.).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The total number of AI publications is not explicitly provided in the given portion of the document. However, it is mentioned that the number of AI publications has more than doubled since 2010 and ranges from thousands to almost 500,000 in 2021, depending on the type of publication and collaboration. Some charts and figures provide specific numbers for certain types of publications, but there is no one definitive answer to the question of how many AI publications.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI # using gpt-3.5 turbo\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "chain = load_qa_chain(llm=ChatOpenAI(), chain_type=\"map_reduce\")\n",
    "query = \"How many AI publications?\"\n",
    "chain.run(input_documents=documents, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20deeb2d-23cd-410b-b189-a78a5b0da98e",
   "metadata": {},
   "source": [
    "### if there is a set of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd2104-e1f2-40f8-9668-69fac3cbeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For multiple documents \n",
    "loaders = [....]\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7783c584-4114-406f-9f80-f4301a755e58",
   "metadata": {},
   "source": [
    "### But if the document is supper long that it exceeds the token limit?\n",
    "\n",
    "Solution 1: Chain Type\n",
    "\n",
    "The default `chain_type=\"stuff\"` uses ALL of the text from the documents in the prompt. It actually doesn’t work with our example because it exceeds the token limit and causes rate-limiting errors. That’s why in this example, we had to use other chain types for example \"map_reduce\". What are the other chain types?\n",
    "\n",
    "- `map_reduce`: It separates texts into batches (as an example, you can define batch size in llm=OpenAI(batch_size=5)), feeds each batch with the question to LLM separately, and comes up with the final answer based on the answers from each batch. 它将文本分成批次（例如，您可以在 llm=OpenAI(batch_size=5) 中定义批次大小），将每个批次与问题分别提供给 LLM，并根据每个批次的答案得出最终答案.并发可能触发`RateLimitError`错误\n",
    "- `refine` : It separates texts into batches, feeds the first batch to LLM, and feeds the answer and the second batch to LLM. It refines the answer by going through all the batches. 它将文本分成批次，将第一批提供给 LLM，然后再将答案和第二批提供给 LLM。它通过遍历所有批次来优化答案。\n",
    "- `map-rerank`: It separates texts into batches, feeds each batch to LLM, returns a score of how fully it answers the question, and comes up with the final answer based on the high-scored answers from each batch. 它将文本分成批次，将每批次提供给 LLM，返回它回答问题的完整程度的分数，并根据每批次的高分答案得出最终答案。\n",
    "\n",
    "Solution 2: RetrievalQA\n",
    "\n",
    "One issue with using ALL of the text is that it can be very costly because you are feeding all the texts to OpenAI API and the API is charged by the number of tokens. A better solution is to retrieve relevant text chunks first and only use the relevant text chunks in the language model. I’m going to go through the details of RetrievalQA next. 使用所有文本的一个问题是它可能非常昂贵，因为您将所有文本提供给 OpenAI API，而 API 按令牌数量收费。更好的解决方案是先检索相关的文本块，并只使用语言模型中的相关文本块。接下来我将详细介绍 RetrievalQA。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580bd3c-e2cb-4ca4-84a2-dd05991d49ad",
   "metadata": {},
   "source": [
    "## Method 2: RetrievalQA\n",
    "\n",
    "`RetrievalQA` chain actually uses `load_qa_chain` under the hood. We retrieve the most relevant chunk of text and feed those to the language model. `RetrievalQA` 链实际上在底层使用了 `load_qa_chain`。我们检索最相关的文本块并将其提供给语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e5e8a-8c8d-45ab-9edb-9fd0d8da733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# create the vectorestore to use as the index\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2}) # defined k as 2 meaning that we are only interested in getting two relevant text chunks.\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "query = \"How many AI publications in 2021?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea6f62d-399f-43f3-80ac-6c56cee03910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How many AI publications in 2021?',\n",
       " 'result': 'According to the context, the total number of AI conference publications in 2021 was 85,094, and the total number of AI publications (including journal articles, conference papers, repositories, and patents) was almost 500,000.',\n",
       " 'source_documents': [Document(page_content='Chapter 1 Preview 17\\nArtificial Intelligence\\nIndex Report 2023\\nAI Conference Publications\\nOverview\\nThe number of AI conference publications peaked in 2019, and fell 20.4% below the peak in 2021 (Figure 1.1.13). \\nThe total number of 2021 AI conference publications, 85,094, was marginally greater than the 2010 total of 75,592.1.1 PublicationsChapter 1: Research and Development\\n85.09\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021020406080100Number of AI Conference Publications (in Thousands)Number of AI Conference Publications, 2010–21\\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report\\nFigure 1.1.13', metadata={'source': 'data/example.pdf', 'page': 16}),\n",
       "  Document(page_content='Chapter 1 Preview 5\\n496.01\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210100200300400500Number of AI Publications (in Thousands)Number of AI Publications in the World, 2010–21 \\nSource: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report Artificial Intelligence\\nIndex Report 2023\\nOverview\\nThe figures below capture the total number \\nof English-language and Chinese-language AI \\npublications globally from 2010 to 2021—by type, \\naffiliation, cross-country collaboration, and cross-\\nindustry collaboration. The section also breaks down 1.1 Publications\\npublication and citation data by region for AI journal \\narticles, conference papers, repositories, and patents.\\nTotal Number of AI Publications\\nFigure 1.1.1 shows the number of AI publications in \\nthe world. From 2010 to 2021, the total number of \\nAI publications more than doubled, growing from \\n200,000 in 2010 to almost 500,000 in 2021.\\n1 See the Appendix for more information on CSET’s methodology. For more on the challenge of defining AI and correctly capturing relevant bibliometric data, see the AI Index team’s \\ndiscussion in the paper “Measurement in AI Policy: Opportunities and Challenges.”This section draws on data from the Center for Security and Emerging Technology (CSET) at Georgetown University. CSET maintains a \\nmerged corpus of scholarly literature that includes Digital Science’s Dimensions, Clarivate’s Web of Science, Microsoft Academic Graph, \\nChina National Knowledge Infrastructure, arXiv, and Papers With Code. In that corpus, CSET applied a classifier to identify English-\\nlanguage publications related to the development or application of AI and ML since 2010. For this year’s report, CSET also used select \\nChinese AI keywords to identify Chinese-language AI papers; CSET did not deploy this method for previous iterations of the AI Index report.1\\nIn last year’s edition of the report, publication trends were reported up to the year 2021. However, given that there is a significant lag in the \\ncollection of publication metadata, and that in some cases it takes until the middle of any given year to fully capture the previous year’s \\npublications, in this year’s report, the AI Index team elected to examine publication trends only through 2021, which we, along with CSET, \\nare confident yields a more fully representative report.1.1 PublicationsChapter 1: Research and Development\\nFigure 1.1.1', metadata={'source': 'data/example.pdf', 'page': 4})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64363371-d3c9-47ff-b34b-4f6d31b002d1",
   "metadata": {},
   "source": [
    "**Options**:\n",
    "There are various options for you to choose from in this process:\n",
    "\n",
    "- [embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html): In the example, we used OpenAI Embeddings. But there are many other embedding options such as Cohere Embeddings, and HuggingFaceEmbeddings from specific models.\n",
    "- [TextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html): We used Character Text Splitter in the example where the text is split by a single character. You can also different text splitters and different tokens mentioned in this doc.\n",
    "- [VectorStore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html): We used Chroma as our vector database where we store our embedded text vectors. Other popular options are FAISS, Mulvus, and Pinecone.\n",
    "- [Retrievers](https://python.langchain.com/en/latest/modules/indexes/retrievers.html): We used a VectoreStoreRetriver, which is backed by a VectorStore. To retrieve text, there are two search types you can choose: search_type: “similarity” or “mmr”. search_type=\"similarity\" uses similarity search in the retriever object where it selects text chunk vectors that are most similar to the question vector. search_type=\"mmr\" uses the maximum marginal relevance search where it optimizes for similarity to query AND diversity among selected documents.\n",
    "- [Chain Type](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html): same as method 1. You can also define the chain type as one of the four options: “stuff”, “map reduce”, “refine”, “map_rerank”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cc0b8-1c4a-4740-8a22-4831c88994c7",
   "metadata": {},
   "source": [
    "## Method 3: VectorstoreIndexCreator\n",
    "\n",
    "`VectorstoreIndexCreator` is a wrapper around the above functionality. It is exactly the same under the hood, but just exposes a higher-level interface to let you get started in three lines of code:\n",
    "\n",
    "`VectorstoreIndexCreator` 是上述功能的包装器。它底层的实现是完全一样的，只是暴露了一个更高级的接口，让你在三行代码中开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64cb7a68-e6cf-4326-8df2-5e523c4bdb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The total number of AI publications has more than doubled from 200,000 in 2010 to almost 500,000 in 2021, according to Figure 1.1.1 in the Artificial Intelligence Index Report 2023.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "query = \"What's the total number in AI publications?\"\n",
    "index.query(llm=ChatOpenAI(), question=query, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56fd77-7fd7-4d55-88cd-5b74ac1b4704",
   "metadata": {},
   "source": [
    "![figure 1.1.1](img/figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268bb994-3fef-45d2-9ea4-19e792607e92",
   "metadata": {},
   "source": [
    "`VectorstoreIndexCreator` 可以指定里面的相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8af1dd97-a6c0-47f5-a206-3a9b57c0cee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The total number of AI publications globally from 2010 to 2021 was almost 500,000, according to Figure 1.1.1 in Chapter 1 Preview 5.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=10000, chunk_overlap=0),\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    vectorstore_cls=Chroma,\n",
    "    vectorstore_kwargs={\"k\":2} # defined k as 2 meaning that we are only interested in getting two relevant text chunks.\n",
    ").from_loaders([loader])\n",
    "query = \"What's the total number of AI pulblications?\"\n",
    "index.query(llm=ChatOpenAI(), question=query, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12438abc-da70-4d4f-ac53-d3ce7f9df268",
   "metadata": {},
   "source": [
    "## Method 4: ConversationalRetrievalChain\n",
    "\n",
    "ConversationalRetrievalChain is very similar to method 2 RetrievalQA. It added an additional parameter chat_history to pass in chat history which can be used for follow-up questions.\n",
    "\n",
    "ConversationalRetrievalChain = conversation memory + RetrievalQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e46c16e4-f0d1-4572-838a-a63d669bb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# load ddocuments\n",
    "loader = PyPDFLoader(\"data/example.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Splite the ddocuments into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# select which embbeddings we want to use \n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# create the vectorstore to use as index\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# expose this index in a Retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "\n",
    "# create a chain to answer the quesion\n",
    "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(), retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26e0847f-fe03-4ec1-9562-f51d3e347b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"What's the total number of AI publications?\"\n",
    "result = qa({\"question\":query, \"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b6ce465-3d9d-4844-af8b-518c20a7d6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What's the total number of AI publications?\",\n",
       " 'chat_history': [],\n",
       " 'answer': 'The total number of AI publications increased from 200,000 in 2010 to almost 500,000 in 2021.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd6a31ed-1d36-47a7-a2b5-c45caf4f019e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The total number of AI publications increased from 200,000 in 2010 to almost 500,000 in 2021.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86b0b623-24bc-4002-8c99-f88000a04e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[(query, result['answer'])]\n",
    "query = \"What's this number divided by 2?\"\n",
    "result = qa({\"question\":query, \"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92bd1f8a-14a7-4238-a418-9fa97e789049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"What's the total number of AI publications?\",\n",
       "  'The total number of AI publications increased from 200,000 in 2010 to almost 500,000 in 2021.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27e4f5eb-03fe-411a-9367-821c71388316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What's this number divided by 2?\",\n",
       " 'chat_history': [(\"What's the total number of AI publications?\",\n",
       "   'The total number of AI publications increased from 200,000 in 2010 to almost 500,000 in 2021.')],\n",
       " 'answer': 'Half of the total number of AI publications is approximately 250,000.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd144a-fae4-4044-a4d8-29e1da987cab",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Now you know four ways to do question answering with LLMs in LangChain. In summary, \n",
    "\n",
    "- load_qa_chain uses all texts and accepts multiple documents;\n",
    "- RetrievalQA uses load_qa_chain under the hood but retrieves relevant text chunks first;\n",
    "- VectorstoreIndexCreator is the same as RetrievalQA with a higher-level interface;\n",
    "- ConversationalRetrievalChain is useful when you want to pass in your chat history to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12731d33-4eb7-4d29-9a50-f42db03851c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
